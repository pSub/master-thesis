module generate-fof

imports libstratego-lib
imports utils
imports include/SLTC

signature
	//FIXME: What is the advantage of defining the sorts explicitly
	sorts Prop Type FOF File
	constructors
		False__ : Prop
    	True__  : Prop
    	Not__   : Prop -> Prop
    	And__   : Prop * Prop -> Prop
    	Or__    : Prop * Prop -> Prop
    	Impl__  : List(Prop) * Prop -> Prop
    	BiImpl__ : Prop * Prop -> Prop
    	Eq__    : Prop * Prop -> Prop
    	Neq__ : Prop * Prop -> Prop
    	Var__   : String -> Prop                   // variables
    	Var__   : String * String -> Prop          // variables annotated with a scope
    	Pred__  : String -> Prop                   // Constants
    	Pred__  : String * List(Prop) -> Prop      // Predicates consisting of a name and a list of arguments
    	Term__  : String -> Prop                   // Terms
    	Term__  : String * List(Prop) -> Prop      // Terms
    	All__   : List(Var) * Prop -> Prop
    	Exists__ : List(Var) * Prop -> Prop
    	
    	Axiom__ : Type
    	Conjecture__ : Type
    	
    	Formula__ : String * Type * Prop -> FOF
    	Formula__ : String * Prop -> FOF
    	FOFGoal__ : List(Formula) * List(Formula) * Formula -> File
		
		AuxCons__ : String * String * List(Prop) -> Prop
    	
    	
// TODO: Abstract from the concret representation of the output format (e.g. to genrate latex files)
// 1) Identifiy the relevant places
// 2) Extract them into a separate module
// 3) Combine the modules
//
// It would be the best thing, if most of the characteristics of the output format would
// be present in the pretty-printing file.

// Relevant parts:
// 1) Renaming of all variables (DONE)
// 2) Separation into multiple files 

 rules
	makeConjectures : Module(ModuleDec(_),
			                 imports,
			                 Language(l),
			                 MetaVariables(m),
			                 Contexts(ContextDefinitions),
			                 Judgments(j),
			                 Rules(TypingRules),
			                 Some(Conjectures(Conjectures)))
			          -> <map(\conjecture -> FOFGoal__(<concat> [ context-formulas
			    												, helper
			    												, <makeConjecture; makeInequalities> conjecture
			    												],
			                                            typing-rule-formulas,
			                                            <makeConjecture> conjecture)
			                  \)> Conjectures
        where
            context-formulas := <map(makeContexts); flatten-list> ContextDefinitions;
            typing-rule-formulas := <map(rewriteTypingRule)> TypingRules;
            
            // Create the injectivity and univalence formulas
            helper := <collect-all(?Term__(_, _)); nub;
            		   mapconcat(\Term__(name, args) -> [ <makeInjectivity> (name, <length> args)
            		   			                        ,  <makeUnivalence> (name, <length> args)
            		   			                        ]
            		   			 \)> typing-rule-formulas


    // Auxilary function that allows 'pointfree' notation. 
    makeNeq : (x,y) -> Neq__(x,y)
    
    // Collects all Terms of the programming language that will result in constants in the first-order representation
    // and generates axioms that say that all those that are syntacically different are indeed different.
    // Those axioms are needed, because as soon as terms get nested those equalities are hard to decide.
	makeInequalities : ast -> <collect-all(?Term__(_)); nub; tails; mapconcat(makeInequality)> ast
	makeInequality : [] -> []
	makeInequality : [y | ys] -> <map(\x -> Formula__("ineq", Axiom__(), <makeNeq>(x, y))\)> ys
    
    //TODO: Is the formulation important? What about using the contraposition?
    //FIXME: It seems those are not used at all. Check why that is and remove if they are superfluous.
    makeInjectivity : (name, n) -> Formula__(<conc-strings> (name, "-injective"),
    									     Axiom__(),
									         All__(<conc>(Xs, Ys), Impl__(<zip; map(makeNeq)> (Xs, Ys),
									                                      Neq__(Term__(name, Xs), Term__(name, Ys))))
    									     )
    	where Xs := <newnames(|"X")> n;
    		  Ys := <newnames(|"Y")> n
    makeUnivalence : (name, n) -> Formula__(<conc-strings> (name, "-univalent"),
    									    Axiom__(),
									        All__(["Y" | ["Z" | Xs]], Impl__([Eq__(Term__(name, Xs), Var__("Y")), Eq__(Term__(name, Xs), Var__("Z"))],
									              Eq__(Var__("Y"), Var__("Z"))))
    									    )
    	where Xs := <newnames(|"X")> n
    

    // Collect free variables
    free-vars : v@Var__(_) -> [<strip-annos> v]
    free-vars : v@Var__(_,_) -> [<strip-annos> v]
    free-vars : All__(vars, prop) -> <diff> (<free-vars> prop, <free-vars> vars)
    free-vars : Exists__(vars, prop) -> <diff> (<free-vars> prop, <free-vars> vars)
    free-vars : cons#(xs) -> <map(free-vars); unions> xs
	
	// Generate context definitions
	makeContexts : ContextDefinition(name, raw-non-terminals) -> [
	    Formula__("lookup base",
	              Axiom__(),
	              All__([NAME|<conc>(inputs, outputs)],
	                    Pred__(lookup, <conc> (inputs, outputs, [Pred__(bind, <conc> (non-terminals, [NAME]))])))),
	    Formula__("lookup step",
	              Axiom__(),
	              All__([NAME|<concat> [inputs, inputs2, outputs, outputs2]],
	                    Impl__([Pred__(lookup, <concat> [inputs, outputs, [NAME]]) | <zip; map(makeNeq)> (inputs, inputs2)],
	                           Pred__(lookup, <conc>(inputs, outputs, [Pred__(bind, <conc> (<map(append(|"_"))> non-terminals, [NAME]))])))))
	]
	   where
	       // Index non-terminals with new names
	       indexed-non-terminals := <zip> (<length; newnames(|"X")> raw-non-terminals, raw-non-terminals);
		   // extract the list of new names
	       non-terminals := <map(Fst; \s -> Var__(s)\)> indexed-non-terminals;
	       // filter out the in- and outputs
	       inputs := <filter(?(_, Hole(_, "I")); Fst; \s -> Var__(s)\)> indexed-non-terminals;
	       outputs := <filter(?(_, Hole(_, "O")); Fst; \s -> Var__(s)\)> indexed-non-terminals;
	       // duplicate the in-and outputs by appending underscores
	       inputs2 := <map(append(|"_"))> inputs;
	       outputs2 := <map(append(|"_"))> outputs;
	       
	       // Auxiliary definitions
	       lookup := <conc-strings> ("lookup", name);
	       bind := <conc-strings> ("bind", name);
	       NAME := Var__(<upper-case> name)
           append(|p) = \ Var__(s) -> Var__(<conc-strings> (s, p)) \

	       
    // Translate all generic construtors into an auxillary data structure
    makeAuxCons : Constructor#(Args) -> AuxCons__(cons, name, Args)
        where
            [cons, name] := <string-tokenize(|['-'])> Constructor;
            <is-generic> cons
            is-generic = "ContextEmpty" <+
            			 "ContextBind" <+
            			 "ContextLookup" <+
            			 "TypingJudgment" <+
            			 "MetaVariable"
    
    // Meta-variables
    rewriteMetaVariable : AuxCons__("MetaVariable", scope, name) -> Var__(<concat-strings> name, scope)
	
	// Contexts in typing rules
	rewriteContextEmpty : AuxCons__("ContextEmpty", name, []) -> Pred__(<concat-strings> ["empty", name])
	rewriteContextBind : AuxCons__("ContextBind", name, args) -> Pred__(<concat-strings> ["bind", name], args)
	rewriteContextLookup : AuxCons__("ContextLookup", name, args) -> Pred__(<concat-strings> ["lookup", name], args)
	rewriteContext = rewriteContextEmpty <+ rewriteContextBind <+ rewriteContextLookup

	// Typing judgments
	rewriteTypingJudgment : AuxCons__("TypingJudgment", "Neq", [arg1, arg2]) -> Neq__(arg1, arg2)
	rewriteTypingJudgment : AuxCons__("TypingJudgment", "Eq", [arg1, arg2]) -> Eq__(arg1, arg2)
	rewriteTypingJudgment : AuxCons__("TypingJudgment", "Not", [arg]) -> Not__(arg)
	rewriteTypingJudgment : AuxCons__("TypingJudgment", name, args) -> Pred__(<conc-strings>("'tcheck", name, "'"), args)
			
	// Typing rules
	rewriteTypingRule : TypingRule(RuleName(_, Name), consequence) -> Formula__(<option-to-string-newname(|"formula")> Name, Axiom__(), prop)
	   with // Rewrite consequence and collect all free variables
	   	    consequence' := <rewrite> consequence;
	        free-vars-in-rule := <free-vars> consequence';
	        // if there are non free variables omit quantification as
	        // it is illegal in TPTP to quantify over no variables.
	        prop := <if [] then <id> consequence' else <id> All__(free-vars-in-rule, consequence') end> free-vars-in-rule
	        
	rewriteTypingRule : TypingRule(PremiseList(premises), RuleName(_, Name), consequence) 
	                       -> Formula__(<option-to-string-newname(|"formula")> Name, Axiom__(), prop)
	   with // Rewrite consequence and premises and collect all free variables
	   	    consequence' := <rewrite> consequence;
            premises' := <to-list(PremiseBase, PremiseCons); rewrite> premises;
	        free-vars-in-rule := <union> (<free-vars> premises', <free-vars> consequence');
	        implication := Impl__(premises', consequence');
			// if there are non free variables omit quantification as
	        // it is illegal in TPTP to quantify over no variables.
	        prop := <if [] then <id> implication else <id> All__(free-vars-in-rule, implication) end> free-vars-in-rule
	        
	// Conjectures
	makeConjecture : TypingRule(RuleName(_, Name), consequence) 
	                   -> Formula__(<option-to-string-newname(|"goal")> Name, Conjecture__(), consequence')
       with consequence' := <rewrite> consequence            
    makeConjecture : TypingRule(PremiseList(premises), RuleName(_, Name), consequence) 
                       -> Formula__(<option-to-string-newname(|"goal")> Name, Conjecture__(), implication)
       with consequence' := <rewrite> consequence;
            premises' := <to-list(PremiseBase, PremiseCons); rewrite> premises;
            implication := Impl__(premises', consequence')

    // Wrap strings into terms
	wrapString : v -> Term__(v)
	   where <is-string> v

	// Wrap all strings that are parameters of Term__ or Pred__ into terms
	rewriteString : Term__(n, args) -> Term__(n, <map(wrapString <+ id)> args)
	rewriteString : Pred__(n, args) -> Pred__(n, <map(wrapString <+ id)> args)
	
	// Wrap the constructors of the programming language into terms
	makeTerm : v -> v
	   where <is-string> v
	makeTerm : cons#([]) -> Term__(<lower-case> cons)
	   where <not([])> cons
    makeTerm : cons#(args) -> Term__(<lower-case> cons, args)
       where <not([])> cons;
             not(<conc-strings; (prop-constructor, "__")> cons)
       prop-constructor = "All" <+ "Not" <+ "And" <+ "Or"   <+ "Impl" <+ "BiImpl" <+ "Formula" <+
                          "Eq"  <+ "Neq" <+ "Var" <+ "Pred" <+ "Term" <+ "All"    <+ "Exists"

strategies
    // Main strategy that apply all transformations to create valid TPT formulas
	toFOF = makeConjectures; makeVariablesUnique; removeDashes

    // Try to apply all rules that rewrite auxilary constructors
	rewriteAuxCons = rewriteContext <+ rewriteTypingJudgment <+ rewriteMetaVariable
	
	// Rewrite the AST topdown. If an auxilary construtor can be created rewrite it immediatly,
	// otherwise try to create a term. After the first topdown traversal, traverse the AST again
	// topdown to rewrite all relevant strings into terms.
	rewrite = topdown(makeAuxCons < rewriteAuxCons + try(makeTerm)); topdown(try(rewriteString))
	
	// Ensure that all variables are uniquely named and are valid TPTP names. Before that run all Variables are scoped, thus
	// all scoped variables are collected. After this the AST is folded over the collected variables. In the AST the scoped
	// variable nodes are replaced with unscoped fresh variables, variable by variable.
	makeVariablesUnique = (fanout; (collect-all(?Var__(_,_)), id)) < foldl(\(v, f) -> <replace(|v, Var__(<newname> "X"), f)> \) 
	                                                               + fail
	
	// FIXME: Remove all dashes from constructor names
	removeDashes = id